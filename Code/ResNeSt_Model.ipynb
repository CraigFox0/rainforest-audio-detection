{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83f7e37",
   "metadata": {},
   "source": [
    "### Imports and Definition of Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8495c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from resnest.torch import resnest50\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rng_seed = 1234\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "torch.cuda.manual_seed(rng_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "num_species = 24\n",
    "batch_size = 64\n",
    "\n",
    "fft = 2048\n",
    "hop = 512 \n",
    "# According to research, standard sampling bitrate is 48khz. Seen in discussion of kaggle competition as well. \n",
    "sr = 48000\n",
    "length = 10*sr\n",
    "\n",
    "data_path = '../Data/'\n",
    "\n",
    "# should change this according to nvidia-smi output e.g. \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda')\n",
    "\n",
    "tp = pd.read_csv(data_path + 'train_tp.csv')\n",
    "fp = pd.read_csv(data_path + 'train_fp.csv')\n",
    "fp['species_id'] = fp['species_id'].apply(lambda x : -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1db41",
   "metadata": {},
   "source": [
    "### Creating Melspectrograms specifically for ResNeSt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d7a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel_spectograms(df, df2, n_fft, hop_length, sample_rate):\n",
    "    \n",
    "    # returns list of tensors representing spectrograms\n",
    "    \n",
    "    mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(power=2.0, sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    tensors = list()\n",
    "    \n",
    "    for idx,row in df.iterrows():\n",
    "\n",
    "        wav, sr = torchaudio.load(data_path + 'train/' + row['recording_id'] + '.flac')\n",
    "        \n",
    "        # Slicing and centering spectograms \n",
    "        m = (int)((row['t_min'] + row['t_max'])*sr/2)\n",
    "    \n",
    "        l = (int)(m-(length/2))\n",
    "        r = (int)(m+(length/2))\n",
    "    \n",
    "        #Assumes audio files are at least as long as length\n",
    "        if l < 0:\n",
    "            r += l\n",
    "            l = 0\n",
    "        elif r > len(wav):\n",
    "            l -= r-len(wav)\n",
    "            r = len(wav)\n",
    "            \n",
    "        melspec = mel_spectrogram_transform(wav[int(l):int(r)])\n",
    "                \n",
    "        tensors.append(melspec)\n",
    "        \n",
    "        \n",
    "    for idx,row in df2.iterrows():\n",
    "#         wav, sr = librosa.load(data_path + 'train/' + row['recording_id'] + '.flac', sr=None)\n",
    "        wav, sr = torchaudio.load(data_path + 'train/' + row['recording_id'] + '.flac')\n",
    "    \n",
    "       # Slicing and centering spectograms \n",
    "        m = (int)((row['t_min'] + row['t_max'])*sr/2)\n",
    "    \n",
    "        l = (int)(m-(length/2))\n",
    "        r = (int)(m+(length/2))\n",
    "    \n",
    "        #Assumes audio files are at least as long as length\n",
    "        if l < 0:\n",
    "            r += l\n",
    "            l = 0\n",
    "        elif r > len(wav):\n",
    "            l -= r-len(wav)\n",
    "            r = len(wav)\n",
    "        \n",
    "#         melspec = librosa.power_to_db(librosa.feature.melspectrogram(y=wav[int(l):int(r)], sr=sr))\n",
    "        melspec = mel_spectrogram_transform(wav[int(l):int(r)])\n",
    "        tensors.append(melspec)\n",
    "        \n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e8a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = create_mel_spectograms(tp, fp, fft, hop, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082b64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([fp, tp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400825be",
   "metadata": {},
   "source": [
    "### Loading in Preproccessed Mel Spectrograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772e99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to2DArray(x): \n",
    "    # casts object representation of specs stored in csv to a 2D numpy array \n",
    "    x=x.replace(\"[\", '')\n",
    "    x=x.replace(\"]\", '')\n",
    "    x=x.replace(\"...\", '')\n",
    "    x=x.replace(\"\\n\", '')\n",
    "    y=np.array(x.split(\" \"))\n",
    "    y = y[y != \"\"]\n",
    "    y = np.asfarray(y, 'float64')\n",
    "    y = np.reshape(y,(1, y.size))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79f40a",
   "metadata": {},
   "source": [
    "### Dataset Definition and Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainforestDataset(Dataset):\n",
    "    def __init__(self, df, list_of_tensors):\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # additional preprocessing required for ResNeST, normalization outlined in paper\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        # custom label encoding to encourage the model to learn from false positive data and not be too confident\n",
    "        # in predictions\n",
    "        labels = df['species_id']\n",
    "        for label in labels:\n",
    "            label_arr = np.full(24, .043478)\n",
    "            if label < 0:\n",
    "                label_arr[label] = 0\n",
    "            else:\n",
    "                label_arr[label] = 1\n",
    "            self.labels.append(label_arr)\n",
    "             \n",
    "        mspecs = list_of_tensors\n",
    "        \n",
    "        for i in range(len(mspecs)):\n",
    "            current_mspec = mspecs[i]\n",
    "            image_mspec = (transforms.ToPILImage()(current_mspec)).convert('RGB')\n",
    "            preprocessed_mspec = self.preprocess(image_mspec)\n",
    "            self.data.append(preprocessed_mspec)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8084a",
   "metadata": {},
   "source": [
    "### Model Definition and Loading to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9d89b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class definition \n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "# ResNeST pretrained model should be uploaded to this path with the notebook\n",
    "model.load_state_dict(torch.load(data_path + 'resnest50-528c19ca.pth'))\n",
    "model.eval()\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "# load model into GPU\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "loss_function.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d779a11",
   "metadata": {},
   "source": [
    "### Definition of Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc89ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, val_loader, model, optimizer, scheduler, loss_function, e_poch):\n",
    "    best_validation = float('inf')\n",
    "\n",
    "    for e in range(0, e_poch):\n",
    "        train_loss = []\n",
    "        \n",
    "        model.train()\n",
    "        for batch, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            output = output.cuda()\n",
    "            \n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr']\n",
    "\n",
    "        print(\"Epoch: \", str(e))\n",
    "        print(\"Learning Rate: \", str(lr))\n",
    "        print(\"Training Loss: \", str(sum(train_loss) / len(train_loss)))\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            val_corr = []\n",
    "\n",
    "            model.eval()\n",
    "            for batch, (data, target) in enumerate(val_loader):\n",
    "                data = data.float()\n",
    "                if torch.cuda.is_available():\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                vals, answers = torch.max(output, 1)\n",
    "                vals, targets = torch.max(target, 1)\n",
    "                corrects = 0\n",
    "                for i in range(0, len(answers)):\n",
    "                    if answers[i] == targets[i]:\n",
    "                        corrects = corrects + 1\n",
    "                val_corr.append(corrects)\n",
    "        \n",
    "        validation_loss = (sum(val_loss) / len(val_loss))\n",
    "\n",
    "        print(\"Epoch: \", str(e))\n",
    "        print(\"Learning Rate: \", str(lr))\n",
    "        print(\"Validation Loss: \", str(validation_loss))\n",
    "\n",
    "\n",
    "        if validation_loss < best_validation:\n",
    "            print('Saving new best model at epoch ' + str(e) + ' (' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n",
    "            torch.save(model, 'new_best_model_resnest.pth')\n",
    "            best_validation = validation_loss\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    del model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ce473",
   "metadata": {},
   "source": [
    "### Creating Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e18c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = None\n",
    "val_df = None\n",
    "\n",
    "X = df.drop('species_id', axis=1)\n",
    "y = df['species_id']\n",
    "\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, random_state=rng_seed, test_size=.2)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19d0d8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ade88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_poch = 20\n",
    "train_dataset = RainforestDataset(train_df, tensors)\n",
    "val_dataset = RainforestDataset(val_df, tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262d4441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  0.19556427630571518\n",
      "Epoch:  0\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  0.19514412417221874\n",
      "Saving new best model at epoch 0 (1565/1800)\n",
      "Epoch:  1\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  0.19547568089040243\n",
      "Epoch:  1\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  0.1946375986090105\n",
      "Saving new best model at epoch 1 (1565/1800)\n",
      "Epoch:  2\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  0.1953922952649959\n",
      "Epoch:  2\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  0.1956746716755553\n",
      "Epoch:  3\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  0.19538059293610763\n",
      "Epoch:  3\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  0.19462992689975478\n",
      "Saving new best model at epoch 3 (1565/1800)\n",
      "Epoch:  4\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  0.19532688470517584\n",
      "Epoch:  4\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  0.19500620693308635\n",
      "Epoch:  5\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  0.195330130054589\n",
      "Epoch:  5\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  0.19451531739171396\n",
      "Saving new best model at epoch 5 (1565/1800)\n",
      "Epoch:  6\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  0.1953491719313398\n",
      "Epoch:  6\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  0.19542180989297983\n",
      "Epoch:  7\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  0.1952589001678808\n",
      "Epoch:  7\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  0.1954896059689596\n",
      "Epoch:  8\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  0.19521724454542236\n",
      "Epoch:  8\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  0.19449666675758634\n",
      "Saving new best model at epoch 8 (1565/1800)\n",
      "Epoch:  9\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  0.195200076985624\n",
      "Epoch:  9\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  0.19500154684522933\n",
      "Epoch:  10\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  0.19519304580113225\n",
      "Epoch:  10\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  0.19502153593825183\n",
      "Epoch:  11\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  0.1952504667334932\n",
      "Epoch:  11\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  0.19498289317777778\n",
      "Epoch:  12\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  0.19517305508257446\n",
      "Epoch:  12\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  0.1949930752768627\n",
      "Epoch:  13\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  0.19522465376629677\n",
      "Epoch:  13\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  0.1958148805419915\n",
      "Epoch:  14\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  0.19509830594344016\n",
      "Epoch:  14\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  0.1944170719699643\n",
      "Saving new best model at epoch 14 (1565/1800)\n",
      "Epoch:  15\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  0.19529782867003476\n",
      "Epoch:  15\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  0.19444859933037362\n",
      "Epoch:  16\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  0.19520196308098278\n",
      "Epoch:  16\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  0.19442249317814034\n",
      "Epoch:  17\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  0.19512415706996214\n",
      "Epoch:  17\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  0.19493617829108462\n",
      "Epoch:  18\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  0.19519876367679312\n",
      "Epoch:  18\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  0.19493693011183186\n",
      "Epoch:  19\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  0.1951446468745741\n",
      "Epoch:  19\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  0.19444701160718542\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, sampler = RandomSampler(train_dataset))\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, sampler = RandomSampler(val_dataset))\n",
    "\n",
    "training_loop(train_loader, val_loader, model, optimizer, scheduler, loss_function, e_poch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1c1f7",
   "metadata": {},
   "source": [
    "### Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edf5a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel_spectograms(file):\n",
    "    fft = 2048\n",
    "    hop = 512 \n",
    "    # According to research, standard sampling bitrate is 48khz. Seen in discussion of kaggle competition as well. \n",
    "    sr = 48000\n",
    "    mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(power=2.0, sample_rate=sr, n_fft=fft, hop_length=hop)\n",
    "    preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(320),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    wav, sr = torchaudio.load(data_path + 'test/{}'.format(file))\n",
    "    \n",
    "    \n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / (10*sr)\n",
    "    segments = int(np.ceil(segments))\n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        melspec = mel_spectrogram_transform(wav)\n",
    "        \n",
    "        image_mspec = (transforms.ToPILImage()(melspec)).convert('RGB')\n",
    "        \n",
    "        preprocessed_mspec = preprocess(image_mspec).numpy()\n",
    "\n",
    "        mel_array.append(preprocessed_mspec)\n",
    "    \n",
    "    return mel_array\n",
    "#     wav, sr = librosa.load(data_path + \"test/{}\".format(file), sr=None)\n",
    "\n",
    "#     mel_spec = librosa.feature.melspectrogram(wav, n_fft=fft, hop_length=hop, sr=sr) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7047657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n",
      "Starting prediction loop\n",
      "1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75250/2904367835.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:207.)\n",
      "  data = torch.tensor(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted for 100 of 1993 files\n",
      "Predicted for 200 of 1993 files\n",
      "Predicted for 300 of 1993 files\n",
      "Predicted for 400 of 1993 files\n",
      "Predicted for 500 of 1993 files\n",
      "Predicted for 600 of 1993 files\n",
      "Predicted for 700 of 1993 files\n",
      "Predicted for 800 of 1993 files\n",
      "Predicted for 900 of 1993 files\n",
      "Predicted for 1000 of 1993 files\n",
      "Predicted for 1100 of 1993 files\n",
      "Predicted for 1200 of 1993 files\n",
      "Predicted for 1300 of 1993 files\n",
      "Predicted for 1400 of 1993 files\n",
      "Predicted for 1500 of 1993 files\n",
      "Predicted for 1600 of 1993 files\n",
      "Predicted for 1700 of 1993 files\n",
      "Predicted for 1800 of 1993 files\n",
      "Predicted for 1900 of 1993 files\n",
      "Submission generated\n"
     ]
    }
   ],
   "source": [
    "# Model class definition \n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "torch.load('new_best_model_resnest.pth')\n",
    "model.eval()\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "\n",
    "# load model into GPU\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    model.cuda()\n",
    "    \n",
    "# Prediction loop\n",
    "print('Starting prediction loop')\n",
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "    \n",
    "    test_files = os.listdir(data_path + 'test/') \n",
    "    print(len(test_files))\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in range(0, len(test_files)):\n",
    "        data = create_mel_spectograms(test_files[i])\n",
    "        data = torch.tensor(data)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Taking max prediction from all slices per bird species\n",
    "        # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "        # In this competition only relative ranking matters, and not the exact value of prediction, so we can use it directly\n",
    "        maxed_output = torch.max(output, dim=0)[0]\n",
    "        maxed_output = maxed_output.cpu().detach()\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "        for out in maxed_output:\n",
    "            write_array.append(out.item())\n",
    "    \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "print('Submission generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c84d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
