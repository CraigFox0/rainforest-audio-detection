{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3840e8e7",
   "metadata": {},
   "source": [
    "## Installing ResNest\n",
    "\n",
    "Pip install of resnest is currently borked, need to download pretrained model manually\n",
    "according to the github, model is stored as ../Data/resnest*.pth on github and loaded below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c537d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install resnest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f7e37",
   "metadata": {},
   "source": [
    "## Imports and Definition of Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8495c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from resnest.torch import resnest50\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skimage.transform import resize\n",
    "import csv\n",
    "\n",
    "rng_seed = 1234\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "torch.cuda.manual_seed(rng_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "num_species = 24\n",
    "batch_size = 8\n",
    "\n",
    "fft = 2048\n",
    "hop = 512 \n",
    "# According to research, standard sampling bitrate is 48khz. Seen in discussion of kaggle competition as well. \n",
    "sr = 48000\n",
    "length = 10*sr\n",
    "\n",
    "# ResNeSt50 input layer is 224 x 224 x 3, specifying dimensions here  \n",
    "mel_spec_dimensions = (224,224)\n",
    "\n",
    "data_path = '../Data/'\n",
    "\n",
    "# should change this according to nvidia-smi output e.g. \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400825be",
   "metadata": {},
   "source": [
    "## Loading in Preproccessed Mel Spectrograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "772e99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to2DArray(x): \n",
    "    # casts object representation of specs stored in csv to a 2D numpy array \n",
    "    x=x.replace(\"[\", '')\n",
    "    x=x.replace(\"]\", '')\n",
    "    x=x.replace(\"...\", '')\n",
    "    x=x.replace(\"\\n\", '')\n",
    "    y=np.array(x.split(\" \"))\n",
    "    y = y[y != \"\"]\n",
    "    y = np.asfarray(y, 'float64')\n",
    "    y = np.reshape(y,(1, y.size))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb824ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>recording_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>songtype_id</th>\n",
       "      <th>t_min</th>\n",
       "      <th>f_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>f_max</th>\n",
       "      <th>mspec_db</th>\n",
       "      <th>chroma_db</th>\n",
       "      <th>stft_db</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>003bec244</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>44.5440</td>\n",
       "      <td>2531.250</td>\n",
       "      <td>45.1307</td>\n",
       "      <td>5531.25</td>\n",
       "      <td>[[-43.52676276, -42.22672291, -40.10429537, -2...</td>\n",
       "      <td>[[ -4.77050225 -15.83399347 -10.23624425 ...  ...</td>\n",
       "      <td>[[-38.94561364 -38.17916253 -36.60923968 ... -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>006ab765f</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>39.9615</td>\n",
       "      <td>7235.160</td>\n",
       "      <td>46.0452</td>\n",
       "      <td>11283.40</td>\n",
       "      <td>[[-20.40886579, -17.40162276, -18.72003747, -5...</td>\n",
       "      <td>[[-2.48446742 -2.87657713 -1.8470297  ... -4.0...</td>\n",
       "      <td>[[-23.07769726 -19.43518602 -19.32889517 ... -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>007f87ba2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>39.1360</td>\n",
       "      <td>562.500</td>\n",
       "      <td>42.2720</td>\n",
       "      <td>3281.25</td>\n",
       "      <td>[[-54.39303891, -55.47439706, -60.4253212, -23...</td>\n",
       "      <td>[[-10.80191837  -5.18153825  -2.21800291 ...  ...</td>\n",
       "      <td>[[-34.0305794  -37.85167318 -42.5896169  ... -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0099c367b</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>51.4206</td>\n",
       "      <td>1464.260</td>\n",
       "      <td>55.1996</td>\n",
       "      <td>4565.04</td>\n",
       "      <td>[[-10.84098544, -14.24867814, -13.64287614, -1...</td>\n",
       "      <td>[[-1.95534572 -1.56999388 -3.34319785 ... -0.8...</td>\n",
       "      <td>[[-17.41080114 -19.76300067 -18.89967515 ... -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>009b760e6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0854</td>\n",
       "      <td>947.461</td>\n",
       "      <td>52.5293</td>\n",
       "      <td>10852.70</td>\n",
       "      <td>[[-19.11522228, -19.91873376, -17.13115801, -4...</td>\n",
       "      <td>[[-1.77162691 -0.72739473 -0.8605218  ... -1.1...</td>\n",
       "      <td>[[-23.5923663  -24.55557959 -23.3306552  ... -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 recording_id  species_id  songtype_id    t_min     f_min  \\\n",
       "0           0    003bec244          14            1  44.5440  2531.250   \n",
       "1           1    006ab765f          23            1  39.9615  7235.160   \n",
       "2           2    007f87ba2          12            1  39.1360   562.500   \n",
       "3           3    0099c367b          17            4  51.4206  1464.260   \n",
       "4           4    009b760e6          10            1  50.0854   947.461   \n",
       "\n",
       "     t_max     f_max                                           mspec_db  \\\n",
       "0  45.1307   5531.25  [[-43.52676276, -42.22672291, -40.10429537, -2...   \n",
       "1  46.0452  11283.40  [[-20.40886579, -17.40162276, -18.72003747, -5...   \n",
       "2  42.2720   3281.25  [[-54.39303891, -55.47439706, -60.4253212, -23...   \n",
       "3  55.1996   4565.04  [[-10.84098544, -14.24867814, -13.64287614, -1...   \n",
       "4  52.5293  10852.70  [[-19.11522228, -19.91873376, -17.13115801, -4...   \n",
       "\n",
       "                                           chroma_db  \\\n",
       "0  [[ -4.77050225 -15.83399347 -10.23624425 ...  ...   \n",
       "1  [[-2.48446742 -2.87657713 -1.8470297  ... -4.0...   \n",
       "2  [[-10.80191837  -5.18153825  -2.21800291 ...  ...   \n",
       "3  [[-1.95534572 -1.56999388 -3.34319785 ... -0.8...   \n",
       "4  [[-1.77162691 -0.72739473 -0.8605218  ... -1.1...   \n",
       "\n",
       "                                             stft_db  \n",
       "0  [[-38.94561364 -38.17916253 -36.60923968 ... -...  \n",
       "1  [[-23.07769726 -19.43518602 -19.32889517 ... -...  \n",
       "2  [[-34.0305794  -37.85167318 -42.5896169  ... -...  \n",
       "3  [[-17.41080114 -19.76300067 -18.89967515 ... -...  \n",
       "4  [[-23.5923663  -24.55557959 -23.3306552  ... -...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_path + 'csv/train_tp_data.csv')\n",
    "df['mspec_db'] = df['mspec_db'].apply(lambda x: to2DArray(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79f40a",
   "metadata": {},
   "source": [
    "## Dataset Definition and Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70688296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainforestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # additional preprocessing required for ResNeST, normalization outlined in paper\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "                \n",
    "        labels = df['species_id'].to_array()\n",
    "        for label in labels:\n",
    "            label_arr = np.zeros(24, dtype=np.single)\n",
    "            label_arr[label] = 1\n",
    "            self.labels.append(label_arr)\n",
    "             \n",
    "        mspecs = df['mspec_db']\n",
    "        \n",
    "        for i in range(len(mspecs)):\n",
    "            current_mspec = (Image.fromarray(mspecs[i])).convert('RGB')\n",
    "            current_mspec = self.preprocess(current_mspec)\n",
    "            self.data.append(current_mspec)\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8084a",
   "metadata": {},
   "source": [
    "## Model Definition and Loading to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9d89b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class definition \n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "# ResNeST pretrained model should be uploaded to this path with the notebook\n",
    "model.load_state_dict(torch.load(data_path + 'resnest50-528c19ca.pth'))\n",
    "model.eval()\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "pos_weight = (torch.ones(num_species) * num_species)\n",
    "\n",
    "# load model into GPU\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight)\n",
    "\n",
    "loss_function.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d779a11",
   "metadata": {},
   "source": [
    "## Definition of Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc89ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, val_loader, model, optimizer, scheduler, pos_weight, loss_function):\n",
    "    best_corrects = 0\n",
    "\n",
    "\n",
    "    for e in range(0, 20):\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        for batch, (data, target) in enumerate(train_loader):\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            output = output.cuda()\n",
    "            \n",
    "            loss = loss_function(output, target)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr']\n",
    "\n",
    "        print(\"Epoch: \", str(e))\n",
    "        print(\"Learning Rate: \", str(lr))\n",
    "        print(\"Training Loss: \", str(sum(train_loss) / len(train_loss)))\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            val_corr = []\n",
    "\n",
    "            model.eval()\n",
    "            for batch, (data, target) in enumerate(val_loader):\n",
    "                data = data.float()\n",
    "                if torch.cuda.is_available():\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                vals, answers = torch.max(output, 1)\n",
    "                vals, targets = torch.max(target, 1)\n",
    "                corrects = 0\n",
    "                for i in range(0, len(answers)):\n",
    "                    if answers[i] == targets[i]:\n",
    "                        corrects = corrects + 1\n",
    "                val_corr.append(corrects)\n",
    "\n",
    "\n",
    "        print(\"Epoch: \", str(e))\n",
    "        print(\"Learning Rate: \", str(lr))\n",
    "        print(\"Validation Loss: \", str(sum(val_loss) / len(val_loss)))\n",
    "\n",
    "\n",
    "        if sum(val_corr) > best_corrects:\n",
    "            print('Saving new best model at epoch ' + str(e) + ' (' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n",
    "            torch.save(model, data_path + 'best_model_resnest.pt')\n",
    "            best_corrects = sum(val_corr)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    del model\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e18c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Learning Rate:  0.001\n",
      "Training Loss:  6.631200467285357\n",
      "Epoch:  0\n",
      "Learning Rate:  0.001\n",
      "Validation Loss:  4.261406390290511\n",
      "Saving new best model at epoch 0 (50/608)\n",
      "Epoch:  1\n",
      "Learning Rate:  0.001\n",
      "Training Loss:  4.232210689469388\n",
      "Epoch:  1\n",
      "Learning Rate:  0.001\n",
      "Validation Loss:  8.304376363754272\n",
      "Epoch:  2\n",
      "Learning Rate:  0.001\n",
      "Training Loss:  4.23956668063214\n",
      "Epoch:  2\n",
      "Learning Rate:  0.001\n",
      "Validation Loss:  5.422140096363268\n",
      "Epoch:  3\n",
      "Learning Rate:  0.001\n",
      "Training Loss:  4.216808400656047\n",
      "Epoch:  3\n",
      "Learning Rate:  0.001\n",
      "Validation Loss:  149.1436058847528\n",
      "Epoch:  4\n",
      "Learning Rate:  0.001\n",
      "Training Loss:  4.214287139867482\n",
      "Epoch:  4\n",
      "Learning Rate:  0.001\n",
      "Validation Loss:  7.570223425564013\n",
      "Epoch:  5\n",
      "Learning Rate:  0.001\n",
      "Training Loss:  4.221502592689113\n",
      "Epoch:  5\n",
      "Learning Rate:  0.001\n",
      "Validation Loss:  4.1888581326133325\n",
      "Epoch:  6\n",
      "Learning Rate:  0.001\n",
      "Training Loss:  4.197579085826874\n",
      "Epoch:  6\n",
      "Learning Rate:  0.001\n",
      "Validation Loss:  9.0743475462261\n",
      "Epoch:  7\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  4.188962613281451\n",
      "Epoch:  7\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  7.454154422408656\n",
      "Epoch:  8\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  4.173438640017259\n",
      "Epoch:  8\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  6.921307444572449\n",
      "Epoch:  9\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  4.173616675954116\n",
      "Epoch:  9\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  8.870926254674009\n",
      "Epoch:  10\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  4.176369133748506\n",
      "Epoch:  10\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  7.4124618705950285\n",
      "Epoch:  11\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  4.1662934234267786\n",
      "Epoch:  11\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  6.404858407221343\n",
      "Epoch:  12\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  4.172307328173988\n",
      "Epoch:  12\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  8.00301128312161\n",
      "Epoch:  13\n",
      "Learning Rate:  0.0004\n",
      "Training Loss:  4.166996886855678\n",
      "Epoch:  13\n",
      "Learning Rate:  0.0004\n",
      "Validation Loss:  7.124619609431217\n",
      "Epoch:  14\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  4.157395723618959\n",
      "Epoch:  14\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  8.902241242559333\n",
      "Epoch:  15\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  4.15730597470936\n",
      "Epoch:  15\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  7.022285248103895\n",
      "Epoch:  16\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  4.157004717149232\n",
      "Epoch:  16\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  9.220228182642083\n",
      "Epoch:  17\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  4.1656600393747025\n",
      "Epoch:  17\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  9.241482207649632\n",
      "Epoch:  18\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  4.157833820895145\n",
      "Epoch:  18\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  7.608337364698711\n",
      "Epoch:  19\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  4.158291559470327\n",
      "Epoch:  19\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  9.189368398565994\n",
      "Epoch:  0\n",
      "Learning Rate:  0.00016\n",
      "Training Loss:  4.160207804880645\n",
      "Epoch:  0\n",
      "Learning Rate:  0.00016\n",
      "Validation Loss:  9.567557083932977\n",
      "Saving new best model at epoch 0 (50/608)\n",
      "Epoch:  1\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  4.156081049065841\n",
      "Epoch:  1\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  8.641791042528654\n",
      "Epoch:  2\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  4.151238595184527\n",
      "Epoch:  2\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  8.213866898888035\n",
      "Epoch:  3\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  4.151748597621918\n",
      "Epoch:  3\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  7.686351518881948\n",
      "Epoch:  4\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  4.151244583882783\n",
      "Epoch:  4\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  8.135266969078465\n",
      "Epoch:  5\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  4.155050980417352\n",
      "Epoch:  5\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  8.294607262862357\n",
      "Epoch:  6\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  4.155633775811446\n",
      "Epoch:  6\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  9.19907680310701\n",
      "Epoch:  7\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Training Loss:  4.149483771700608\n",
      "Epoch:  7\n",
      "Learning Rate:  6.400000000000001e-05\n",
      "Validation Loss:  8.898423596432334\n",
      "Epoch:  8\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  4.149229454366784\n",
      "Epoch:  8\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  8.327227479533144\n",
      "Epoch:  9\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  4.147009573484722\n",
      "Epoch:  9\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  9.210836159555535\n",
      "Epoch:  10\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  4.151185380785089\n",
      "Epoch:  10\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  8.107332204517565\n",
      "Epoch:  11\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  4.148037305003719\n",
      "Epoch:  11\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  8.355235815048218\n",
      "Epoch:  12\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  4.147391818071666\n",
      "Epoch:  12\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  8.39629938727931\n",
      "Epoch:  13\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  4.149890981222454\n",
      "Epoch:  13\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  7.141452538339715\n",
      "Epoch:  14\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Training Loss:  4.154930952348207\n",
      "Epoch:  14\n",
      "Learning Rate:  2.5600000000000006e-05\n",
      "Validation Loss:  7.75638336884348\n",
      "Epoch:  15\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Training Loss:  4.159301444103844\n",
      "Epoch:  15\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Validation Loss:  7.824219816609433\n",
      "Epoch:  16\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Training Loss:  4.146767584901107\n",
      "Epoch:  16\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Validation Loss:  8.987215631886533\n",
      "Epoch:  17\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Training Loss:  4.146368352990401\n",
      "Epoch:  17\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Validation Loss:  9.631615375217638\n",
      "Epoch:  18\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Training Loss:  4.147253582352086\n",
      "Epoch:  18\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Validation Loss:  8.971537150834736\n",
      "Epoch:  19\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Training Loss:  4.153825386574394\n",
      "Epoch:  19\n",
      "Learning Rate:  1.0240000000000004e-05\n",
      "Validation Loss:  7.767897787847017\n"
     ]
    }
   ],
   "source": [
    "train_df = None\n",
    "val_df = None\n",
    "\n",
    "X = df.drop('species_id', axis=1)\n",
    "y = df['species_id']\n",
    "\n",
    "strat = StratifiedKFold(n_splits=2, shuffle=True, random_state=rng_seed)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(strat.split(X,y)):\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[val_index]\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    train_dataset = RainforestDataset(train_df)\n",
    "    val_dataset = RainforestDataset(val_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, sampler = RandomSampler(train_dataset))\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size, sampler = RandomSampler(val_dataset))\n",
    "    training_loop(train_loader, val_loader, model, optimizer, scheduler, pos_weight, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d7a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel_spectograms(df):\n",
    "    wav, sr = librosa.load(data_path + \"test/{}\".format(df), sr=None)\n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        mel_spec = librosa.feature.melspectrogram(slice, n_fft=fft, hop_length=hop, sr=sr)\n",
    "        mel_spec = resize(mel_spec, mel_spec_dimensions)\n",
    "    \n",
    "        mel_spec = mel_spec - np.min(mel_spec)\n",
    "        mel_spec = mel_spec / np.max(mel_spec)\n",
    "        \n",
    "        mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "\n",
    "        mel_array.append(mel_spec)\n",
    "    \n",
    "    return mel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d39c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction loop\n",
      "1992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58727/1666734208.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mel_spectograms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = torch.load(data_path + 'best_model_resnest.pt')\n",
    "model.eval()\n",
    "\n",
    "# Scoring does not like many files:(\n",
    "#if save_to_disk == 0:\n",
    "#    for f in os.listdir('/kaggle/working/'):\n",
    "#        os.remove('/kaggle/working/' + f)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "# Prediction loop\n",
    "print('Starting prediction loop')\n",
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "    \n",
    "    test_files = os.listdir(data_path + 'test/') \n",
    "    print(len(test_files))\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in range(0, len(test_files)):\n",
    "        data = create_mel_spectograms(test_files[i])\n",
    "        data = torch.tensor(data)\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Taking max prediction from all slices per bird species\n",
    "        # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "        # In this competition only relative ranking matters, and not the exact value of prediction, so we can use it directly\n",
    "        maxed_output = torch.max(output, dim=0)[0]\n",
    "        maxed_output = maxed_output.cpu().detach()\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "        for out in maxed_output:\n",
    "            write_array.append(out.item())\n",
    "    \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "print('Submission generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dee42e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
