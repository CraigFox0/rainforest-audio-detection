{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba842dda-e473-4329-aa3b-e819bd1e05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import soundfile as sf \n",
    "import librosa\n",
    "from skimage.transform import resize \n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import random \n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader \n",
    "import torch.utils.data as td\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.utils.data as td \n",
    "import csv\n",
    "# Setting seeds for reproducible results \n",
    "rng_seed = 1234\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "torch.cuda.manual_seed(rng_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "num_species = 24\n",
    "batch_size = 8\n",
    "\n",
    "fft = 2048\n",
    "hop = 512 \n",
    "# According to research, standard sampling bitrate is 48khz. Seen in discussion of kaggle competition as well. \n",
    "sr = 48000\n",
    "length = 10*sr\n",
    "# ResNet50 input layer is 224 x 224 x 3, so I'm resizing the image to fit the first input dimension. \n",
    "mel_spec_dimensions = (224,224)\n",
    "\n",
    "data_path = '../Data/'\n",
    "\n",
    "df = pd.read_csv(data_path + 'train_Augmented.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e58d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tuk99233/rainforest-audio-detection/Code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edfe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audf = pd.read_csv('https://raw.githubusercontent.com/CraigFox0/rainforest-audio-detection/main/Data/csv/train_tp_data.csv')\n",
    "#!curl 'https://raw.githubusercontent.com/CraigFox0/rainforest-audio-detection/main/Data/csv/train_tp_data.csv' > savedFile.txt\n",
    "#https://github.com/CraigFox0/rainforest-audio-detection/blob/main/Data/csv/train_tp_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9e988",
   "metadata": {},
   "source": [
    "### Cuda Device Selection\n",
    "\n",
    "Use cuda:{device_num} to select cuda device that is not being used already\n",
    "\n",
    "Make sure that this device is selected by exporting CUDA_VISIBLE_DEVICES={device_num} on the shell that's running the notebook server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e449b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d85c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e38d5-436f-4a9d-bdb0-4f16058010a9",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790ae1c-8353-4566-9d52-f482a341ca8d",
   "metadata": {},
   "source": [
    "Just using a quick technique for now, worried only about getting model results back. Will use different preprocessing steps as we move on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735bff2-4e1c-4699-b638-683f152f6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_mel_spectograms(df):\n",
    "#     df['spec'] = np.nan\n",
    "#     df['spec'] = df['spec'].astype(object)\n",
    "    \n",
    "#     for idx,row in df.iterrows():\n",
    "        \n",
    "#         rid = row['recording_id']\n",
    "\n",
    "#         wav, sr = librosa.load(data_path + 'train/' + rid + '.flac', sr=None)\n",
    "\n",
    "         # Slicing and centering spectograms \n",
    "#         m = np.round((row['t_min'] + row['t_max']) / 2)\n",
    "#         l = m - length / 2\n",
    "#         if l < 0: l = 0\n",
    "#         r = m + length\n",
    "#         if r > len(wav):\n",
    "#             r = len(wav)\n",
    "#             l = r - m\n",
    "\n",
    "#         mspec = librosa.feature.melspectrogram(y=wav[int(l):int(r)], n_fft=fft, hop_length=hop, sr=sr)\n",
    "#         mspec = resize(mspec, mel_spec_dimensions)\n",
    "#         mspec = (mspec - np.min(mspec))/np.max(mspec)\n",
    "            \n",
    "#         df.at[idx, 'spec'] = mspec\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e6d49",
   "metadata": {},
   "source": [
    "### Optional: Rerun Mel Spectogram Pipeline \n",
    "\n",
    "Note: should not be necessary if up to date with main branch\n",
    "train_spectograms.csv should already be saved at Data/train_spectograms.csv, though we might need to pickle the spec 2d array so that we can retrieve it for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21cc00a-ad40-4342-815f-9282d66b6f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = create_mel_spectograms(df)\n",
    "# df.to_csv(data_path + 'train_spectograms.csv')\n",
    "# print(df.dtypes)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../../BatchNormalizedAudio.csv') #File Containing the Spectograms as String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e877b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to2DArray(x): \n",
    "    x=x.replace(\"[\", '')\n",
    "    x=x.replace(\"]\", '')\n",
    "    x=x.replace(\"...\", '')\n",
    "    x=x.replace(\"\\n\", '')\n",
    "    y=np.array(x.split(\" \"))\n",
    "    y = y[y != \"\"]\n",
    "    y = np.asfarray(y, 'float64')\n",
    "    y = np.reshape(y,(1, y.size))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['mspec_db'] = df['mspec_db'].apply(lambda x: to2DArray(x))\n",
    "#df['chroma_db'] = df['chroma_db'].apply(lambda x: to2DArray(x))\n",
    "#df['stft_db'] = df['stft_db'].apply(lambda x: to2DArray(x))\n",
    "df['spec'] = df['spec'].apply(lambda x: to2DArray(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b12103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['spec'] = df['spec'].apply(lambda x: to2DArray(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fedf57-304f-41c8-96fd-52e48e42a7b1",
   "metadata": {},
   "source": [
    "### Creating PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dee5c6-600e-4653-9d98-9e8fc2569f35",
   "metadata": {},
   "source": [
    "Note: Have to stack the spectrograms so that they're (224 x 224 x 3) to fit the input dimensions of ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79ff8e-646d-4408-a7c5-081beee62e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFCXDatasetFromArr(td.Dataset):\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "         # need this to transform data to tensors    \n",
    "        self.transform = transforms.ToTensor()\n",
    "                \n",
    "        labels = df['species_id'].to_list()\n",
    "        for label in labels:\n",
    "            label_arr = np.zeros(24, dtype=np.single)\n",
    "            label_arr[label] = 1.\n",
    "            self.labels.append(label_arr)\n",
    "             \n",
    "        specs = df['spec']\n",
    "        #specs = [df['mspec_db'],df['chroma_db'], df['stft_db']]\n",
    "            \n",
    "        for i in range(len(specs)):\n",
    "            current_spec = np.array(specs[i])\n",
    "            stack = np.stack([current_spec, current_spec, current_spec])\n",
    "            self.data.append(stack)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data[idx]), torch.tensor(self.labels[idx]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ef2b9-8a3a-4c54-bfaa-1d2da6a37837",
   "metadata": {},
   "source": [
    "### Creating Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbc71b-5011-4a5f-8a10-088b7fc79298",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = None\n",
    "val_df = None\n",
    "\n",
    "# df = pd.read_csv(data_path + 'train_spectograms.csv')\n",
    "X = df.drop('species_id', axis=1)\n",
    "y = df['species_id']\n",
    "\n",
    "strat = StratifiedKFold(n_splits=5, shuffle=True, random_state=rng_seed) #Use All folds on training loop\n",
    "train_dfs=[]\n",
    "val_dfs=[]\n",
    "for fold, (train_index, val_index) in enumerate(strat.split(X,y)):\n",
    "    #if fold==0:\n",
    "    #    train_df = df.iloc[train_index]\n",
    "   #     val_df = df.iloc[val_index]\n",
    "    #if fold==1:\n",
    "    #    train_df1 = df.iloc[train_index]\n",
    "     #   val_df1 = df.iloc[val_index]\n",
    "    #if fold==2:\n",
    "    #    train_df2 = df.iloc[train_index]\n",
    "    #    val_df2 = df.iloc[val_index]\n",
    "    #if fold==3:\n",
    "    #    train_df3 = df.iloc[train_index]\n",
    "    #    val_df3 = df.iloc[val_index]\n",
    "    #if fold==4:\n",
    "     #   train_df4 = df.iloc[train_index]\n",
    "      #  val_df4 = df.iloc[val_index]\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[val_index]\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    train_dfs.append(train_df)\n",
    "    val_dfs.append(val_df)\n",
    "        \n",
    "\n",
    "#train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "#val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705cf8a-8b5f-4012-98f6-195c702627d5",
   "metadata": {},
   "source": [
    "### Configuring Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fbdb6-cd40-4377-ac64-7e595960ecd5",
   "metadata": {},
   "source": [
    "ResNet50 Research Reference: https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5#data-augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d8d59-d5e4-44f8-b34c-e0478d9fe546",
   "metadata": {},
   "source": [
    "After reading up on ResNet at the above link, SGD was recommended as an optimizer. Went with a recommended learning rate scheduler from a related notebook in Kaggle. The above link recommends a different scheduler. We chose to use BCE w/ Logits Loss also based on recommendations from related work. We plan on trying out multiple different loss functions to see what works best for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c456c6e-a476-4bbb-a4ad-a0d227711ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition \n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "pos_weight = (torch.ones(num_species) * num_species)\n",
    "\n",
    "# load model into GPU\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight)\n",
    "\n",
    "loss_function.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e30d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freqs = pd.DataFrame(df.species_id.value_counts())\n",
    "#num_total = sum(freqs['species_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freqs['species_id'] = freqs['species_id'].apply(lambda x: x/num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4116447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Weighting posititons based on frequency\n",
    "#torch.ones(num_species) * num_species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e913cff-9d7a-4e70-8354-4f852a091d28",
   "metadata": {},
   "source": [
    "Below, we can see the shape of our model. Note that ResNet50 has an output dimension of 2048, which we pass through a fully connected layer. The output of our fc layer is in agreement with competition standards. We designed the FC layer based on related work, and will optimize it in later phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f0efa-98bc-4d2e-ab83-f99dc400c77e",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54d7b8-6d82-4594-bf6a-670e0041cd15",
   "metadata": {},
   "source": [
    "Training loop based on the work of another Kaggle notebook: https://www.kaggle.com/fffrrt/all-in-one-rfcx-baseline-for-beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833d7b5-c255-435b-bb45-683ee6032514",
   "metadata": {},
   "source": [
    "Maintains a validation accuracy statistic (Does the most probable class match the ground-truth label?) as the model trains, and saves the model with the highest validation accuracy to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35eb353-32f5-4ff2-ade3-f2656c42be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, val_loader, model, optimizer, scheduler, pos_weight, loss_function, best_correct, e_poch, model_num):\n",
    "\n",
    "\n",
    "    for e in range(0, e_poch):\n",
    "        train_loss = []\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        for batch, (data, target) in enumerate(train_loader):\n",
    "\n",
    "#             print(data.shape)\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "#                 print(\"Loading training data to device\")\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            output = output.cuda()\n",
    "            \n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr']\n",
    "\n",
    "        print(\"Epoch: \", str(e))\n",
    "        print(\"Learning Rate: \", str(lr))\n",
    "        print(\"Training Loss: \", str(sum(train_loss) / len(train_loss)))\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            val_corr = []\n",
    "\n",
    "            model.eval()\n",
    "            for batch, (data, target) in enumerate(val_loader):\n",
    "                data = data.float()\n",
    "                if torch.cuda.is_available():\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "        \n",
    "                \n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                vals, answers = torch.max(output, 1)\n",
    "                vals, targets = torch.max(target, 1)\n",
    "                corrects = 0\n",
    "                for i in range(0, len(answers)):\n",
    "                    if answers[i] == targets[i]:\n",
    "                        corrects = corrects + 1\n",
    "                val_corr.append(corrects)\n",
    "\n",
    "\n",
    "        print(\"Epoch: \", str(e))\n",
    "        print(\"Learning Rate: \", str(lr))\n",
    "        print(\"Validation Loss: \", str(sum(val_loss) / len(val_loss)))\n",
    "\n",
    "\n",
    "        if sum(val_corr) > best_correct:\n",
    "            print('Saving new best model at epoch ' + str(e) + ' (' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n",
    "            torch.save(model, 'best_model{}.pt'.format(model_num))\n",
    "            best_correct = sum(val_corr)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    del model\n",
    "    \n",
    "    return best_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72a635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_dataset = RFCXDatasetFromArr(train_dfs[x])\n",
    "#val_dataset = RFCXDatasetFromArr(val_dfs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1):\n",
    "    train_dataset = RFCXDatasetFromArr(train_dfs[x])\n",
    "    val_dataset = RFCXDatasetFromArr(val_dfs[x])\n",
    "    best_correct = 0\n",
    "    model_num = x\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, sampler = td.RandomSampler(train_dataset))\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size, sampler = td.RandomSampler(val_dataset))\n",
    "    lr_vals = [.001] #Adjust\n",
    "    epochs = [5] #Adjust\n",
    "    for x in range(len(lr_vals)):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr_vals[x], weight_decay=0.0001, momentum=0.9)\n",
    "        for y in range(len(epochs)):\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n",
    "            best_correct_new = training_loop(train_loader, val_loader, model, optimizer, scheduler, pos_weight, loss_function, best_correct, epochs[y], model_num)\n",
    "            if(best_correct_new > best_correct):\n",
    "                best_correct = best_correct_new\n",
    "                best_epoch = epochs[y]\n",
    "                best_lr = lr_vals[x]\n",
    "    print(\"Best Learning Rate is {}\".format(best_lr))\n",
    "    print(\"Best Epochs is {}\".format(best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc5410",
   "metadata": {},
   "source": [
    "### Submission Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b00554",
   "metadata": {},
   "source": [
    "Based on the work of another Kaggle notebook: https://www.kaggle.com/fffrrt/all-in-one-rfcx-baseline-for-beginners. (Should edit before final project done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_loop(train_loader, model, optimizer, scheduler, pos_weight, loss_function, best_correct, e_poch):\n",
    "\n",
    "\n",
    "    for e in range(0, e_poch):\n",
    "        train_loss = []\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        for batch, (data, target) in enumerate(train_loader):\n",
    "\n",
    "#             print(data.shape)\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "#                 print(\"Loading training data to device\")\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            output = output.cuda()\n",
    "            \n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr']\n",
    "\n",
    "        print(\"Epoch: \", str(e))\n",
    "        print(\"Learning Rate: \", str(lr))\n",
    "        print(\"Training Loss: \", str(sum(train_loss) / len(train_loss)))\n",
    "\n",
    "        if sum(train_loss) > best_correct:\n",
    "            print('Saving new best model at epoch ')\n",
    "            torch.save(model, 'best_model.pt')\n",
    "            best_correct = sum(val_corr)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    del model\n",
    "    \n",
    "    return best_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset2 = RFCXDatasetFromArr(df)\n",
    "#train_loader2 = DataLoader(train_dataset2, batch_size = batch_size, sampler = td.RandomSampler(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.SGD(model.parameters(), lr=best_lr, weight_decay=0.0001, momentum=0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n",
    "#testing_loop(train_loader2, model, optimizer, scheduler, pos_weight, loss_function, best_correct, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RFCXDatasetFromArr(train_dfs[x])\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, sampler = td.RandomSampler(train_dataset))\n",
    "dataloader_iterator = iter(train_loader)\n",
    "data, target = next(dataloader_iterator)\n",
    "model0 = models.resnet50(pretrained=True)\n",
    "model0.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "model0 = torch.load('best_model0.pt')\n",
    "model0.train()\n",
    "#for batch, (data, target) in enumerate(train_loader):\n",
    "\n",
    "#             print(data.shape)\n",
    "data = data.float()\n",
    "if torch.cuda.is_available():\n",
    "#                 print(\"Loading training data to device\")\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = model(data)\n",
    "output = output.cuda()\n",
    "            \n",
    "loss = loss_function(output, target)\n",
    "            \n",
    "            \n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "#train_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel_spectograms(df):\n",
    "    wav, sr = librosa.load(\"test/{}\".format(df), sr=None)\n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        mel_spec = librosa.feature.melspectrogram(slice, n_fft=fft, hop_length=hop, sr=sr)\n",
    "        mel_spec = resize(mel_spec, mel_spec_dimensions)\n",
    "    \n",
    "        mel_spec = mel_spec - np.min(mel_spec)\n",
    "        mel_spec = mel_spec / np.max(mel_spec)\n",
    "        \n",
    "        mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "\n",
    "        mel_array.append(mel_spec)\n",
    "    \n",
    "    return mel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad249f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model0 = models.resnet50(pretrained=True)\n",
    "# model0.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, num_species)\n",
    "# )\n",
    "\n",
    "# model0 = torch.load('best_model0.pt')\n",
    "# model0.eval()\n",
    "\n",
    "# model1 = models.resnet50(pretrained=True)\n",
    "# model1.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, num_species)\n",
    "# )\n",
    "\n",
    "# model1 = torch.load('best_model1.pt')\n",
    "# model1.eval()\n",
    "\n",
    "# model2 = models.resnet50(pretrained=True)\n",
    "# model2.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, num_species)\n",
    "# )\n",
    "\n",
    "# model2 = torch.load('best_model2.pt')\n",
    "# model2.eval()\n",
    "\n",
    "# model3 = models.resnet50(pretrained=True)\n",
    "# model3.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, num_species)\n",
    "# )\n",
    "\n",
    "# model3 = torch.load('best_model3.pt')\n",
    "# model3.eval()\n",
    "\n",
    "# model4 = models.resnet50(pretrained=True)\n",
    "# model4.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, num_species)\n",
    "# )\n",
    "\n",
    "# model4 = torch.load('best_model4.pt')\n",
    "# model4.eval()\n",
    "\n",
    "# # Scoring does not like many files:(\n",
    "# #if save_to_disk == 0:\n",
    "# #    for f in os.listdir('/kaggle/working/'):\n",
    "# #        os.remove('/kaggle/working/' + f)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()\n",
    "    \n",
    "# # Prediction loop\n",
    "# print('Starting prediction loop')\n",
    "# with open('submission.csv', 'w', newline='') as csvfile:\n",
    "#     submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "#     submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "#                                's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "    \n",
    "#     test_files = os.listdir('test/') \n",
    "#     print(len(test_files))\n",
    "    \n",
    "#     # Every test file is split on several chunks and prediction is made for each chunk\n",
    "#     for i in range(0, len(test_files)):\n",
    "#         data = create_mel_spectograms(test_files[i])\n",
    "#         data = torch.tensor(data)\n",
    "#         data = data.float()\n",
    "#         if torch.cuda.is_available():\n",
    "#             data = data.cuda()\n",
    "\n",
    "#         output4 = model4(data)\n",
    "#         output2 = model2(data)\n",
    "#         output3 = model3(data)\n",
    "#         output0 = model0(data)\n",
    "#         output1 = model1(data)\n",
    "#         # Taking max prediction from all slices per bird species\n",
    "#         # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "#         # In this competition only relative ranking matters, and not the exact value of prediction, so we can use it directly\n",
    "#         maxed_output0 = torch.max(output0, dim=0)[0]\n",
    "#         maxed_output0 = maxed_output0.cpu().detach()\n",
    "#         maxed_output1 = torch.max(output1, dim=0)[0]\n",
    "#         maxed_output1 = maxed_output1.cpu().detach()\n",
    "#         maxed_output2 = torch.max(output2, dim=0)[0]\n",
    "#         maxed_output3 = maxed_output2.cpu().detach()\n",
    "#         maxed_output3 = torch.max(output3, dim=0)[0]\n",
    "#         maxed_output3 = maxed_output3.cpu().detach()\n",
    "#         maxed_output4 = torch.max(output4, dim=0)[0]\n",
    "#         maxed_output4 = maxed_output4.cpu().detach()\n",
    "        \n",
    "#         file_id = str.split(test_files[i], '.')[0]\n",
    "#         write_array0 = [file_id]\n",
    "#         write_array1 = [file_id]\n",
    "#         write_array2 = [file_id]\n",
    "#         write_array3 = [file_id]\n",
    "#         write_array4 = [file_id]\n",
    "#         for out in maxed_output0:\n",
    "#             write_array0.append(out.item())\n",
    "#         for out in maxed_output1:\n",
    "#             write_array1.append(out.item())\n",
    "#         for out in maxed_output2:\n",
    "#             write_array2.append(out.item())\n",
    "#         for out in maxed_output3:\n",
    "#             write_array3.append(out.item())\n",
    "#         for out in maxed_output4:\n",
    "#             write_array4.append(out.item())\n",
    "            \n",
    "#         write_array = [file_id]\n",
    "#         for x in range(1, len(write_array0)):\n",
    "#             agg = [write_array0[x], write_array1[x], write_array2[x], write_array3[x], write_array4[x]]\n",
    "#             print(agg)\n",
    "#             write_array.append(statistics.mean(agg))\n",
    "#         submission_writer.writerow(write_array)\n",
    "        \n",
    "#         if i % 100 == 0 and i > 0:\n",
    "#             print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "# print('Submission generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting prediction loop')\n",
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "    \n",
    "    test_files = os.listdir('test/') \n",
    "    #print(len(test_files))\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in range(0, len(test_files)):\n",
    "        data = create_mel_spectograms(test_files[i])\n",
    "        data = torch.tensor(data)\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        #output4 = model4(data)\n",
    "        #output2 = model2(data)\n",
    "        #output3 = model3(data)\n",
    "        output0 = model0(data)\n",
    "        #output1 = model1(data)\n",
    "        # Taking max prediction from all slices per bird species\n",
    "        # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "        # In this competition only relative ranking matters, and not the exact value of prediction, so we can use it directly\n",
    "        maxed_output0 = torch.max(output0, dim=0)[0]\n",
    "        maxed_output0 = maxed_output0.cpu().detach()\n",
    "\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array0 = [file_id]\n",
    "\n",
    "        for out in maxed_output0:\n",
    "            write_array0.append(out.item())\n",
    "            \n",
    "        #write_array = [file_id]\n",
    "        #write_array.append(write_array0)\n",
    "        submission_writer.writerow(write_array0)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "print('Submission generated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
