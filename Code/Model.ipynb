{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba842dda-e473-4329-aa3b-e819bd1e05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import soundfile as sf \n",
    "# !pip install librosa\n",
    "import librosa\n",
    "from skimage.transform import resize \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70afff8-44b3-430e-ac01-c2e44b91797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/train_tp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e9b0f4ce-9d35-40ea-bf40-8d7f20f6899a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>songtype_id</th>\n",
       "      <th>t_min</th>\n",
       "      <th>f_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>f_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003bec244</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>44.5440</td>\n",
       "      <td>2531.250</td>\n",
       "      <td>45.1307</td>\n",
       "      <td>5531.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>006ab765f</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>39.9615</td>\n",
       "      <td>7235.160</td>\n",
       "      <td>46.0452</td>\n",
       "      <td>11283.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007f87ba2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>39.1360</td>\n",
       "      <td>562.500</td>\n",
       "      <td>42.2720</td>\n",
       "      <td>3281.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0099c367b</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>51.4206</td>\n",
       "      <td>1464.260</td>\n",
       "      <td>55.1996</td>\n",
       "      <td>4565.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009b760e6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0854</td>\n",
       "      <td>947.461</td>\n",
       "      <td>52.5293</td>\n",
       "      <td>10852.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recording_id  species_id  songtype_id    t_min     f_min    t_max     f_max\n",
       "0    003bec244          14            1  44.5440  2531.250  45.1307   5531.25\n",
       "1    006ab765f          23            1  39.9615  7235.160  46.0452  11283.40\n",
       "2    007f87ba2          12            1  39.1360   562.500  42.2720   3281.25\n",
       "3    0099c367b          17            4  51.4206  1464.260  55.1996   4565.04\n",
       "4    009b760e6          10            1  50.0854   947.461  52.5293  10852.70"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fft = 2048\n",
    "hop = 512 \n",
    "sr = 48000\n",
    "length = 10*sr\n",
    "\n",
    "df = pd.read_csv('./Data/train_tp.csv')[0:304]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ce47b3e2-6654-43e6-a88a-d29c4ba40f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 train examples from 304\n",
      "Processed 100 train examples from 304\n",
      "Processed 200 train examples from 304\n",
      "Processed 300 train examples from 304\n"
     ]
    }
   ],
   "source": [
    "# Creating bitmap images for each spectogram to run through resnet50\n",
    "\n",
    "# Source: https://www.kaggle.com/fffrrt/all-in-one-rfcx-baseline-for-beginners\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    wav, sr = librosa.load('./Data/train/' + str(row['recording_id']) + '.flac', sr=None)\n",
    "\n",
    "    t_min = row['t_min']*sr\n",
    "    t_max = row['t_max']*sr\n",
    "    \n",
    "    center = np.round((t_min + t_max) / 2) \n",
    "    beginning = center - length / 2 \n",
    "    if beginning < 0: \n",
    "        beginning = 0 \n",
    "    \n",
    "    ending = beginning + length \n",
    "    if ending > len(wav):\n",
    "        ending = len(wav)\n",
    "        beginning = ending - length\n",
    "            \n",
    "    mel_spec = librosa.feature.melspectrogram(y=wav[int(beginning):int(ending)], n_fft=fft, hop_length=hop, sr=sr, fmin=0, fmax=24000, power=1.5)\n",
    "    mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "    # Normalizing spectrogram \n",
    "    mel_spec = (mel_spec - np.min(mel_spec)) / np.max(mel_spec)\n",
    "    \n",
    "    mel_spec = mel_spec * 255\n",
    "    mel_spec = np.round(mel_spec)    \n",
    "    mel_spec = mel_spec.astype('uint8')\n",
    "    mel_spec = np.asarray(mel_spec)\n",
    "    \n",
    "    bmp = Image.fromarray(mel_spec, 'L')\n",
    "    bmp.save('./Data/working/train/' + str(row['recording_id']) + '_' + str(row['species_id']) + '_' + str(center) + '.bmp')\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print('Processed ' + str(idx) + ' train examples from ' + str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "53d17cf6-084f-4ce0-88f0-8587a473f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random \n",
    "\n",
    "num_species = 24\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "rng_seed = 1234\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "torch.cuda.manual_seed(rng_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f1355fd-1ace-496d-8c07-1ff4957052c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as td \n",
    "\n",
    "# Creating torch dataset\n",
    "class RFCXDataset(td.Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for file in files:\n",
    "            # One-hot encoded labels \n",
    "            label = int(str.split(file, '_')[1])\n",
    "            label_arr = np.zeros(num_species, dtype=np.single)\n",
    "            label_arr[label] = 1.\n",
    "            self.labels.append(label_arr)\n",
    "            \n",
    "            # Open and save spectrogram\n",
    "            \n",
    "            img = Image.open('./Data/working/train/' + file)\n",
    "            mel_spec = np.array(img)\n",
    "            img.close()\n",
    "            \n",
    "            # bmp -> (0,1)\n",
    "            mel_spec = mel_spec/255\n",
    "            mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "            \n",
    "            self.data.append(mel_spec)\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item], self.labels[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "af85e975-6da6-4cd3-990d-e3b6f454e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 243 examples\n",
      "Validating on 61 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setting up training and testing split \n",
    "file_list = []\n",
    "label_list = []\n",
    "\n",
    "for f in os.listdir('./Data/working/train'):\n",
    "    if '.bmp' in f:\n",
    "        file_list.append(f)\n",
    "        label = str.split(f, '_')[1]\n",
    "        label_list.append(label)\n",
    "        \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle= True, random_state=rng_seed)\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "for fold_id, (tr_idx, val_idx) in enumerate(skf.split(file_list, label_list)):\n",
    "    \n",
    "    if fold_id == 0: # Just one fold \n",
    "        \n",
    "        train_files = np.take(file_list, tr_idx)\n",
    "        val_files = np.take(file_list, val_idx)\n",
    "        \n",
    "        \n",
    "print('Training on ' + str(len(train_files)) + ' examples')\n",
    "print('Validating on ' + str(len(val_files)) + ' examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "600af583-2483-450b-b341-1e28c33af22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network for baseline \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader \n",
    "# !pip install resnest\n",
    "# !pip install torchdata\n",
    "import torch.utils.data as td\n",
    "from resnest.torch import resnest50\n",
    "import torchvision\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6c456c6e-a476-4bbb-a4ad-a0d227711ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RFCXDataset(train_files)\n",
    "val_dataset = RFCXDataset(val_files)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, sampler = td.RandomSampler(train_dataset))\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, sampler = td.RandomSampler(val_dataset))\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n",
    "\n",
    "pos_weights = torch.ones(num_species)\n",
    "pos_weights = pos_weights * num_species\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight = pos_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f722d4a9-78c8-4797-bed7-a8c125678775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for batch, (data, target) in enumerate(train_loader):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b35eb353-32f5-4ff2-ade3-f2656c42be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 0 training end. LR: 0.01, Loss: 1.3659581292060115, Correct answers: 14/243\n",
      "Epoch 0 validation end. LR: 0.01, Loss: 1.34653802216053, Correct answers: 4/61\n",
      "Saving new best model at epoch 0 (4/61)\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 1 training end. LR: 0.01, Loss: 1.325682678530293, Correct answers: 14/243\n",
      "Epoch 1 validation end. LR: 0.01, Loss: 1.3151792585849762, Correct answers: 6/61\n",
      "Saving new best model at epoch 1 (6/61)\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 2 training end. LR: 0.01, Loss: 1.2442618108564807, Correct answers: 23/243\n",
      "Epoch 2 validation end. LR: 0.01, Loss: 1.1853712648153305, Correct answers: 7/61\n",
      "Saving new best model at epoch 2 (7/61)\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 3 training end. LR: 0.01, Loss: 1.1277371433473402, Correct answers: 26/243\n",
      "Epoch 3 validation end. LR: 0.01, Loss: 1.1823425814509392, Correct answers: 9/61\n",
      "Saving new best model at epoch 3 (9/61)\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 4 training end. LR: 0.01, Loss: 1.0422297716140747, Correct answers: 35/243\n",
      "Epoch 4 validation end. LR: 0.01, Loss: 1.1446578130126, Correct answers: 6/61\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 5 training end. LR: 0.01, Loss: 1.108641707128094, Correct answers: 41/243\n",
      "Epoch 5 validation end. LR: 0.01, Loss: 1.1222598105669022, Correct answers: 11/61\n",
      "Saving new best model at epoch 5 (11/61)\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 6 training end. LR: 0.01, Loss: 0.9807696996196624, Correct answers: 52/243\n",
      "Epoch 6 validation end. LR: 0.01, Loss: 1.1319995000958443, Correct answers: 13/61\n",
      "Saving new best model at epoch 6 (13/61)\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "batch tick\n",
      "Epoch 7 training end. LR: 0.004, Loss: 0.8270721358637656, Correct answers: 70/243\n",
      "Epoch 7 validation end. LR: 0.004, Loss: 0.9974935054779053, Correct answers: 14/61\n",
      "Saving new best model at epoch 7 (14/61)\n"
     ]
    }
   ],
   "source": [
    "best_corrects = 0\n",
    "\n",
    "# Train loop\n",
    "print('Starting training loop')\n",
    "for e in range(0, 8):\n",
    "    # Stats\n",
    "    train_loss = []\n",
    "    train_corr = []\n",
    "    \n",
    "    # Single epoch - train\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        print('batch tick')\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        vals, answers = torch.max(output, 1)\n",
    "        vals, targets = torch.max(target, 1)\n",
    "        corrects = 0\n",
    "        for i in range(0, len(answers)):\n",
    "            if answers[i] == targets[i]:\n",
    "                corrects = corrects + 1\n",
    "        train_corr.append(corrects)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    for g in optimizer.param_groups:\n",
    "        lr = g['lr']\n",
    "    print('Epoch ' + str(e) + ' training end. LR: ' + str(lr) + ', Loss: ' + str(sum(train_loss) / len(train_loss)) +\n",
    "          ', Correct answers: ' + str(sum(train_corr)) + '/' + str(train_dataset.__len__()))\n",
    "    \n",
    "    # Single epoch - validation\n",
    "    with torch.no_grad():\n",
    "        # Stats\n",
    "        val_loss = []\n",
    "        val_corr = []\n",
    "        \n",
    "        model.eval()\n",
    "        for batch, (data, target) in enumerate(val_loader):\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            # Stats\n",
    "            vals, answers = torch.max(output, 1)\n",
    "            vals, targets = torch.max(target, 1)\n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "            val_corr.append(corrects)\n",
    "        \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    print('Epoch ' + str(e) + ' validation end. LR: ' + str(lr) + ', Loss: ' + str(sum(val_loss) / len(val_loss)) +\n",
    "          ', Correct answers: ' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()))\n",
    "    \n",
    "    # If this epoch is better than previous on validation, save model\n",
    "    # Validation loss is the more common metric, but in this case our loss is misaligned with competition metric, making accuracy a better metric\n",
    "    if sum(val_corr) > best_corrects:\n",
    "        print('Saving new best model at epoch ' + str(e) + ' (' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n",
    "        torch.save(model, 'best_model.pt')\n",
    "        best_corrects = sum(val_corr)\n",
    "        \n",
    "    # Call every epoch\n",
    "    scheduler.step()\n",
    "\n",
    "# Free memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c9121e30-10bd-446c-940e-4cdcbd0e767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_file(f):\n",
    "    wav, sr = librosa.load('./Data/test/' + f, sr=None)\n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            sl = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            sl = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        mel_spec = librosa.feature.melspectrogram(y=sl, n_fft=fft, hop_length=hop, sr=sr, power=1.5)\n",
    "        mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "        mel_spec = mel_spec - np.min(mel_spec)\n",
    "        mel_spec = mel_spec / np.max(mel_spec)\n",
    "        \n",
    "        mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "\n",
    "        mel_array.append(mel_spec)\n",
    "    \n",
    "    return mel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7c46ef27-1d54-4113-a8ae-9c82021de87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction\n",
      "1992\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "Predicted for 100 of 1993 files\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "Predicted for 200 of 1993 files\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "Predicted for 300 of 1993 files\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "Predicted for 400 of 1993 files\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "tick\n",
      "Submission Generated\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "model = torch.load('best_model.pt')\n",
    "model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "print(\"Starting prediction\")\n",
    "with open(\"predictions.csv\", 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "    test_files = os.listdir('./Data/test')\n",
    "    print(len(test_files))\n",
    "    \n",
    "    for i in range(0, int(len(test_files)/4)):\n",
    "        data = load_test_file(test_files[i])\n",
    "        print('tick')\n",
    "        data = torch.tensor(data)\n",
    "        print('tick')\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            \n",
    "        output = model(data)\n",
    "        \n",
    "        maxed_output = torch.max(output, dim=0)[0]\n",
    "        maxed_output = maxed_output.cpu().detach()\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "        for out in maxed_output:\n",
    "            write_array.append(out.item())\n",
    "            \n",
    "            \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "    \n",
    "print('Submission Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a455c-0df7-483f-b233-bf86999fd913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
