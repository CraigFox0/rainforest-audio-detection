{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "290485b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali as dali\n",
    "import librosa as librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d83f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed = 1234\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "torch.cuda.manual_seed(rng_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "num_species = 24\n",
    "batch_size = 8\n",
    "\n",
    "fft = 2048\n",
    "hop = 512 \n",
    "# According to research, standard sampling bitrate is 48khz. Seen in discussion of kaggle competition as well. \n",
    "sr = 48000\n",
    "length = 10*sr\n",
    "# ResNet50 input layer is 224 x 224 x 3, so I'm resizing the image to fit the first input dimension. \n",
    "mel_spec_dimensions = (224,224)\n",
    "\n",
    "data_path = '../Data/'\n",
    "\n",
    "df = pd.read_csv(data_path + 'train_tp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3e4fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def spectogram_pipeline(df, idx, device='cpu'):\n",
    "    \n",
    "    rid = df.iloc[idx]['recording_id']\n",
    "    wav, sr = librosa.load(data_path + 'train/' + rid + '.flac', sr=None)\n",
    "    \n",
    "    audio_data = np.array(wav, dtype=np.float64)\n",
    "    \n",
    "    audio = types.Constant(device=device, value=audio_data)\n",
    "    spectrogram = fn.spectrogram(audio, device=device, nfft=fft, \n",
    "                                 window_length=length,\n",
    "                                 window_step=hop)\n",
    "    mel_spectrogram = fn.mel_filter_bank(spectrogram, sample_rate=sr, nfilter = 128, freq_high = 8000.0)\n",
    "    mel_spectrogram_dB = fn.to_decibels(mel_spectrogram, multiplier = 10.0, cutoff_db = -80)\n",
    "    return mel_spectrogram_dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c569a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_spectrogram(spec, title, sr, hop_length, y_axis='log', x_axis='time'):\n",
    "    librosa.display.specshow(spec, sr=sr, y_axis=y_axis, x_axis=x_axis, hop_length=hop_length)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c521f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 10 17:17:24 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   43C    P0   206W / 300W |  16072MiB / 16384MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   44C    P0   209W / 300W |   6923MiB / 16384MiB |     90%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    71W / 300W |  13699MiB / 16384MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   52C    P0   294W / 300W |  13553MiB / 16384MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    88W / 300W |   1016MiB / 16384MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   52C    P0   260W / 300W |  11872MiB / 16384MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   48C    P0   264W / 300W |  13040MiB / 16384MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    63W / 300W |  14921MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     35462      C   python                          11301MiB |\n",
      "|    0   N/A  N/A     62815      C   ...da3/envs/EAST3/bin/python     3753MiB |\n",
      "|    0   N/A  N/A     63971      C   ...3565/anaconda3/bin/python     1013MiB |\n",
      "|    1   N/A  N/A      4567      C   ...onda3/envs/SSL/bin/python     2903MiB |\n",
      "|    1   N/A  N/A     62815      C   ...da3/envs/EAST3/bin/python     4017MiB |\n",
      "|    2   N/A  N/A      9767      C   ./BGMRESv4_gpu                   9607MiB |\n",
      "|    2   N/A  N/A     62815      C   ...da3/envs/EAST3/bin/python     4089MiB |\n",
      "|    3   N/A  N/A     31265      C   python                           9461MiB |\n",
      "|    3   N/A  N/A     62815      C   ...da3/envs/EAST3/bin/python     4089MiB |\n",
      "|    4   N/A  N/A     63971      C   ...3565/anaconda3/bin/python     1013MiB |\n",
      "|    5   N/A  N/A     25042      C   python                          11867MiB |\n",
      "|    6   N/A  N/A     32113      C   python                          13037MiB |\n",
      "|    7   N/A  N/A     17501      C   /opt/conda/bin/python3.8          303MiB |\n",
      "|    7   N/A  N/A     19977      C   python                          14611MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "os.system('nvidia-smi')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7fbd539",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Critical error in pipeline:\nException in CPU stage: [/opt/dali/dali/pipeline/executor/executor.cc:62] Assert on \"device_id_ == CPU_ONLY_DEVICE_ID\" failed: Wrong device_id provided, it should be >= 0, or equal to CPU_ONLY_DEVICE_ID.\nStacktrace (8 entries):\n[frame 0]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0x7e0bf) [0x154d17b820bf]\n[frame 1]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(dali::Executor<dali::AOT_WS_Policy<dali::UniformQueuePolicy>, dali::UniformQueuePolicy>::RunCPUImpl()+0xf1e) [0x154d17bed5be]\n[frame 2]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(dali::Executor<dali::AOT_WS_Policy<dali::UniformQueuePolicy>, dali::UniformQueuePolicy>::RunCPU()+0xe) [0x154d17bed9ae]\n[frame 3]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0xa4951) [0x154d17ba8951]\n[frame 4]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0x119f9c) [0x154d17c1df9c]\n[frame 5]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0x6f508f) [0x154d181f908f]\n[frame 6]: /lib/x86_64-linux-gnu/libpthread.so.0(+0x9609) [0x154d5ff7f609]\n[frame 7]: /lib/x86_64-linux-gnu/libc.so.6(clone+0x43) [0x154d5fd3f293]\n\nCurrent pipeline object is no longer valid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17501/1647481132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectogram_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmel_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshow_spectogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_api_type_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipelineAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py\u001b[0m in \u001b[0;36moutputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batches_to_consume\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gpu_batches_to_consume\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mschedule_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py\u001b[0m in \u001b[0;36m_outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline must be built first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Critical error in pipeline:\nException in CPU stage: [/opt/dali/dali/pipeline/executor/executor.cc:62] Assert on \"device_id_ == CPU_ONLY_DEVICE_ID\" failed: Wrong device_id provided, it should be >= 0, or equal to CPU_ONLY_DEVICE_ID.\nStacktrace (8 entries):\n[frame 0]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0x7e0bf) [0x154d17b820bf]\n[frame 1]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(dali::Executor<dali::AOT_WS_Policy<dali::UniformQueuePolicy>, dali::UniformQueuePolicy>::RunCPUImpl()+0xf1e) [0x154d17bed5be]\n[frame 2]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(dali::Executor<dali::AOT_WS_Policy<dali::UniformQueuePolicy>, dali::UniformQueuePolicy>::RunCPU()+0xe) [0x154d17bed9ae]\n[frame 3]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0xa4951) [0x154d17ba8951]\n[frame 4]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0x119f9c) [0x154d17c1df9c]\n[frame 5]: /opt/conda/lib/python3.8/site-packages/nvidia/dali/libdali.so(+0x6f508f) [0x154d181f908f]\n[frame 6]: /lib/x86_64-linux-gnu/libpthread.so.0(+0x9609) [0x154d5ff7f609]\n[frame 7]: /lib/x86_64-linux-gnu/libc.so.6(clone+0x43) [0x154d5fd3f293]\n\nCurrent pipeline object is no longer valid."
     ]
    }
   ],
   "source": [
    "pipe = spectogram_pipeline(df, 1, num_threads=3, batch_size=1)\n",
    "pipe.build()\n",
    "output = pipe.run()\n",
    "mel_spec = np.array(outputs[0][0].as_cpu())\n",
    "show_spectogram(mel_spec, \"spec\", sr, hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f83c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
